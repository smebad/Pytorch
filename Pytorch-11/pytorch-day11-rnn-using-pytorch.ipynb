{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "392399e0-d1bd-4555-8a79-6814aa17d4b4",
       "rows": [
        [
         "0",
         "What is the capital of France?",
         "Paris"
        ],
        [
         "1",
         "What is the capital of Germany?",
         "Berlin"
        ],
        [
         "2",
         "Who wrote 'To Kill a Mockingbird'?",
         "Harper-Lee"
        ],
        [
         "3",
         "What is the largest planet in our solar system?",
         "Jupiter"
        ],
        [
         "4",
         "What is the boiling point of water in Celsius?",
         "100"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question      answer\n",
       "0                   What is the capital of France?       Paris\n",
       "1                  What is the capital of Germany?      Berlin\n",
       "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
       "3  What is the largest planet in our solar system?     Jupiter\n",
       "4   What is the boiling point of water in Celsius?         100"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('100_Unique_QA_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our data to numbers\n",
    "# tokenization function\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = text.lower() # convert to lowercase\n",
    "    text = text.replace('?', '') # replace ? with nothing\n",
    "    text = text.replace(\"'\", '') # replace ' with nothing \n",
    "    return text.split() # this will return a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'capital', 'of', 'france']\n",
      "['what', 'is', 'the', 'capital', 'of', 'germany']\n",
      "['who', 'wrote', '\"to', 'kill', 'a', 'mockingbird\"']\n",
      "['what', 'is', 'the', 'largest', 'planet', 'in', 'our', 'solar', 'system']\n",
      "['what', 'is', 'the', 'boiling', 'point', 'of', 'water', 'in', 'celsius']\n"
     ]
    }
   ],
   "source": [
    "# testing the tokenization function\n",
    "print(tokenizer('What is the capital of France?'))\n",
    "print(tokenizer('What is the capital of Germany?'))\n",
    "print(tokenizer('Who wrote \"To Kill a Mockingbird\"?'))\n",
    "print(tokenizer('What is the largest planet in our solar system?'))\n",
    "print(tokenizer('What is the boiling point of water in Celsius?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a vocabulary for unique words\n",
    "vocab = {'<UNK>':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to check if there is a unique word\n",
    "def build_vocab(row):\n",
    "  print(row['question'], row['answer'])\n",
    "\n",
    "  tokenized_question = tokenizer(row['question']) # this will return a list of words\n",
    "  tokenized_answer = tokenizer(row['answer'])\n",
    "\n",
    "  # merging the question and answer into one list\n",
    "  tokenized_text = tokenized_question + tokenized_answer\n",
    "\n",
    "  # looping through the list of words\n",
    "  for word in tokenized_text:\n",
    "    if word not in vocab:\n",
    "      vocab[word] = len(vocab) # this will add the word to the vocabulary\n",
    "\n",
    "\n",
    "  #print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France? Paris\n",
      "What is the capital of Germany? Berlin\n",
      "Who wrote 'To Kill a Mockingbird'? Harper-Lee\n",
      "What is the largest planet in our solar system? Jupiter\n",
      "What is the boiling point of water in Celsius? 100\n",
      "Who painted the Mona Lisa? Leonardo-da-Vinci\n",
      "What is the square root of 64? 8\n",
      "What is the chemical symbol for gold? Au\n",
      "Which year did World War II end? 1945\n",
      "What is the longest river in the world? Nile\n",
      "What is the capital of Japan? Tokyo\n",
      "Who developed the theory of relativity? Albert-Einstein\n",
      "What is the freezing point of water in Fahrenheit? 32\n",
      "Which planet is known as the Red Planet? Mars\n",
      "Who is the author of '1984'? George-Orwell\n",
      "What is the currency of the United Kingdom? Pound\n",
      "What is the capital of India? Delhi\n",
      "Who discovered gravity? Newton\n",
      "How many continents are there on Earth? 7\n",
      "Which gas do plants use for photosynthesis? CO2\n",
      "What is the smallest prime number? 2\n",
      "Who invented the telephone? Alexander-Graham-Bell\n",
      "What is the capital of Australia? Canberra\n",
      "Which ocean is the largest? Pacific-Ocean\n",
      "What is the speed of light in vacuum? 299,792,458m/s\n",
      "Which language is spoken in Brazil? Portuguese\n",
      "Who discovered penicillin? Alexander-Fleming\n",
      "What is the capital of Canada? Ottawa\n",
      "What is the largest mammal on Earth? Whale\n",
      "Which element has the atomic number 1? Hydrogen\n",
      "What is the tallest mountain in the world? Everest\n",
      "Which city is known as the Big Apple? NewYork\n",
      "How many planets are in the Solar System? 8\n",
      "Who painted 'Starry Night'? vangogh\n",
      "What is the chemical formula of water? H2O\n",
      "What is the capital of Italy? Rome\n",
      "Which country is famous for sushi? Japan\n",
      "Who was the first person to step on the Moon? Armstrong\n",
      "What is the main ingredient in guacamole? Avocado\n",
      "How many sides does a hexagon have? 6\n",
      "What is the currency of China? Yuan\n",
      "Who wrote 'Pride and Prejudice'? Jane-Austen\n",
      "What is the chemical symbol for iron? Fe\n",
      "What is the hardest natural substance on Earth? Diamond\n",
      "Which continent is the largest by area? Asia\n",
      "Who was the first President of the United States? George-Washington\n",
      "Which bird is known for its ability to mimic sounds? Parrot\n",
      "What is the longest-running animated TV show? Simpsons\n",
      "What is the smallest country in the world? VaticanCity\n",
      "Which planet has the most moons? Saturn\n",
      "Who wrote 'Romeo and Juliet'? Shakespeare\n",
      "What is the main gas in Earth's atmosphere? Nitrogen\n",
      "How many bones are in the adult human body? 206\n",
      "Which metal is a liquid at room temperature? Mercury\n",
      "What is the capital of Russia? Moscow\n",
      "Who discovered electricity? Benjamin-Franklin\n",
      "Which is the second-largest country by land area? Canada\n",
      "What is the color of a ripe banana? Yellow\n",
      "Which month has 28 days in a common year? February\n",
      "What is the study of living organisms called? Biology\n",
      "Which country is home to the Great Wall? China\n",
      "What do bees collect from flowers? Nectar\n",
      "What is the opposite of 'day'? Night\n",
      "What is the capital of South Korea? Seoul\n",
      "Who invented the light bulb? Edison\n",
      "Which gas do humans breathe in for survival? Oxygen\n",
      "What is the square root of 144? 12\n",
      "Which country has the pyramids of Giza? Egypt\n",
      "Which sea creature has eight arms? Octopus\n",
      "Which holiday is celebrated on December 25? Christmas\n",
      "What is the currency of Japan? Yen\n",
      "How many legs does a spider have? 8\n",
      "Which sport uses a net, ball, and hoop? Basketball\n",
      "Which country is famous for its kangaroos? Australia\n",
      "Who was the first female Prime Minister of the UK? MargaretThatcher\n",
      "Which is the fastest land animal? Cheetah\n",
      "What is the first element on the periodic table? Hydrogen\n",
      "What is the capital of Spain? Madrid\n",
      "Which planet is the closest to the Sun? Mercury\n",
      "Who is known as the father of computers? CharlesBabbage\n",
      "What is the capital of Mexico? MexicoCity\n",
      "How many colors are in a rainbow? 7\n",
      "Which musical instrument has black and white keys? Piano\n",
      "Who discovered the Americas in 1492? ChristopherColumbus\n",
      "Which Disney character has a long nose and grows it when lying? Pinocchio\n",
      "Who directed the movie 'Titanic'? JamesCameron\n",
      "Which superhero is also known as the Dark Knight? Batman\n",
      "What is the capital of Brazil? Brasilia\n",
      "Which fruit is known as the king of fruits? Mango\n",
      "Which country is known for the Eiffel Tower? France\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying pandas apply function for each row\n",
    "df.apply(build_vocab, axis=1) # axis=1 means it will apply the function to each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'what': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'capital': 4,\n",
       " 'of': 5,\n",
       " 'france': 6,\n",
       " 'paris': 7,\n",
       " 'germany': 8,\n",
       " 'berlin': 9,\n",
       " 'who': 10,\n",
       " 'wrote': 11,\n",
       " 'to': 12,\n",
       " 'kill': 13,\n",
       " 'a': 14,\n",
       " 'mockingbird': 15,\n",
       " 'harper-lee': 16,\n",
       " 'largest': 17,\n",
       " 'planet': 18,\n",
       " 'in': 19,\n",
       " 'our': 20,\n",
       " 'solar': 21,\n",
       " 'system': 22,\n",
       " 'jupiter': 23,\n",
       " 'boiling': 24,\n",
       " 'point': 25,\n",
       " 'water': 26,\n",
       " 'celsius': 27,\n",
       " '100': 28,\n",
       " 'painted': 29,\n",
       " 'mona': 30,\n",
       " 'lisa': 31,\n",
       " 'leonardo-da-vinci': 32,\n",
       " 'square': 33,\n",
       " 'root': 34,\n",
       " '64': 35,\n",
       " '8': 36,\n",
       " 'chemical': 37,\n",
       " 'symbol': 38,\n",
       " 'for': 39,\n",
       " 'gold': 40,\n",
       " 'au': 41,\n",
       " 'which': 42,\n",
       " 'year': 43,\n",
       " 'did': 44,\n",
       " 'world': 45,\n",
       " 'war': 46,\n",
       " 'ii': 47,\n",
       " 'end': 48,\n",
       " '1945': 49,\n",
       " 'longest': 50,\n",
       " 'river': 51,\n",
       " 'nile': 52,\n",
       " 'japan': 53,\n",
       " 'tokyo': 54,\n",
       " 'developed': 55,\n",
       " 'theory': 56,\n",
       " 'relativity': 57,\n",
       " 'albert-einstein': 58,\n",
       " 'freezing': 59,\n",
       " 'fahrenheit': 60,\n",
       " '32': 61,\n",
       " 'known': 62,\n",
       " 'as': 63,\n",
       " 'red': 64,\n",
       " 'mars': 65,\n",
       " 'author': 66,\n",
       " '1984': 67,\n",
       " 'george-orwell': 68,\n",
       " 'currency': 69,\n",
       " 'united': 70,\n",
       " 'kingdom': 71,\n",
       " 'pound': 72,\n",
       " 'india': 73,\n",
       " 'delhi': 74,\n",
       " 'discovered': 75,\n",
       " 'gravity': 76,\n",
       " 'newton': 77,\n",
       " 'how': 78,\n",
       " 'many': 79,\n",
       " 'continents': 80,\n",
       " 'are': 81,\n",
       " 'there': 82,\n",
       " 'on': 83,\n",
       " 'earth': 84,\n",
       " '7': 85,\n",
       " 'gas': 86,\n",
       " 'do': 87,\n",
       " 'plants': 88,\n",
       " 'use': 89,\n",
       " 'photosynthesis': 90,\n",
       " 'co2': 91,\n",
       " 'smallest': 92,\n",
       " 'prime': 93,\n",
       " 'number': 94,\n",
       " '2': 95,\n",
       " 'invented': 96,\n",
       " 'telephone': 97,\n",
       " 'alexander-graham-bell': 98,\n",
       " 'australia': 99,\n",
       " 'canberra': 100,\n",
       " 'ocean': 101,\n",
       " 'pacific-ocean': 102,\n",
       " 'speed': 103,\n",
       " 'light': 104,\n",
       " 'vacuum': 105,\n",
       " '299,792,458m/s': 106,\n",
       " 'language': 107,\n",
       " 'spoken': 108,\n",
       " 'brazil': 109,\n",
       " 'portuguese': 110,\n",
       " 'penicillin': 111,\n",
       " 'alexander-fleming': 112,\n",
       " 'canada': 113,\n",
       " 'ottawa': 114,\n",
       " 'mammal': 115,\n",
       " 'whale': 116,\n",
       " 'element': 117,\n",
       " 'has': 118,\n",
       " 'atomic': 119,\n",
       " '1': 120,\n",
       " 'hydrogen': 121,\n",
       " 'tallest': 122,\n",
       " 'mountain': 123,\n",
       " 'everest': 124,\n",
       " 'city': 125,\n",
       " 'big': 126,\n",
       " 'apple': 127,\n",
       " 'newyork': 128,\n",
       " 'planets': 129,\n",
       " 'starry': 130,\n",
       " 'night': 131,\n",
       " 'vangogh': 132,\n",
       " 'formula': 133,\n",
       " 'h2o': 134,\n",
       " 'italy': 135,\n",
       " 'rome': 136,\n",
       " 'country': 137,\n",
       " 'famous': 138,\n",
       " 'sushi': 139,\n",
       " 'was': 140,\n",
       " 'first': 141,\n",
       " 'person': 142,\n",
       " 'step': 143,\n",
       " 'moon': 144,\n",
       " 'armstrong': 145,\n",
       " 'main': 146,\n",
       " 'ingredient': 147,\n",
       " 'guacamole': 148,\n",
       " 'avocado': 149,\n",
       " 'sides': 150,\n",
       " 'does': 151,\n",
       " 'hexagon': 152,\n",
       " 'have': 153,\n",
       " '6': 154,\n",
       " 'china': 155,\n",
       " 'yuan': 156,\n",
       " 'pride': 157,\n",
       " 'and': 158,\n",
       " 'prejudice': 159,\n",
       " 'jane-austen': 160,\n",
       " 'iron': 161,\n",
       " 'fe': 162,\n",
       " 'hardest': 163,\n",
       " 'natural': 164,\n",
       " 'substance': 165,\n",
       " 'diamond': 166,\n",
       " 'continent': 167,\n",
       " 'by': 168,\n",
       " 'area': 169,\n",
       " 'asia': 170,\n",
       " 'president': 171,\n",
       " 'states': 172,\n",
       " 'george-washington': 173,\n",
       " 'bird': 174,\n",
       " 'its': 175,\n",
       " 'ability': 176,\n",
       " 'mimic': 177,\n",
       " 'sounds': 178,\n",
       " 'parrot': 179,\n",
       " 'longest-running': 180,\n",
       " 'animated': 181,\n",
       " 'tv': 182,\n",
       " 'show': 183,\n",
       " 'simpsons': 184,\n",
       " 'vaticancity': 185,\n",
       " 'most': 186,\n",
       " 'moons': 187,\n",
       " 'saturn': 188,\n",
       " 'romeo': 189,\n",
       " 'juliet': 190,\n",
       " 'shakespeare': 191,\n",
       " 'earths': 192,\n",
       " 'atmosphere': 193,\n",
       " 'nitrogen': 194,\n",
       " 'bones': 195,\n",
       " 'adult': 196,\n",
       " 'human': 197,\n",
       " 'body': 198,\n",
       " '206': 199,\n",
       " 'metal': 200,\n",
       " 'liquid': 201,\n",
       " 'at': 202,\n",
       " 'room': 203,\n",
       " 'temperature': 204,\n",
       " 'mercury': 205,\n",
       " 'russia': 206,\n",
       " 'moscow': 207,\n",
       " 'electricity': 208,\n",
       " 'benjamin-franklin': 209,\n",
       " 'second-largest': 210,\n",
       " 'land': 211,\n",
       " 'color': 212,\n",
       " 'ripe': 213,\n",
       " 'banana': 214,\n",
       " 'yellow': 215,\n",
       " 'month': 216,\n",
       " '28': 217,\n",
       " 'days': 218,\n",
       " 'common': 219,\n",
       " 'february': 220,\n",
       " 'study': 221,\n",
       " 'living': 222,\n",
       " 'organisms': 223,\n",
       " 'called': 224,\n",
       " 'biology': 225,\n",
       " 'home': 226,\n",
       " 'great': 227,\n",
       " 'wall': 228,\n",
       " 'bees': 229,\n",
       " 'collect': 230,\n",
       " 'from': 231,\n",
       " 'flowers': 232,\n",
       " 'nectar': 233,\n",
       " 'opposite': 234,\n",
       " 'day': 235,\n",
       " 'south': 236,\n",
       " 'korea': 237,\n",
       " 'seoul': 238,\n",
       " 'bulb': 239,\n",
       " 'edison': 240,\n",
       " 'humans': 241,\n",
       " 'breathe': 242,\n",
       " 'survival': 243,\n",
       " 'oxygen': 244,\n",
       " '144': 245,\n",
       " '12': 246,\n",
       " 'pyramids': 247,\n",
       " 'giza': 248,\n",
       " 'egypt': 249,\n",
       " 'sea': 250,\n",
       " 'creature': 251,\n",
       " 'eight': 252,\n",
       " 'arms': 253,\n",
       " 'octopus': 254,\n",
       " 'holiday': 255,\n",
       " 'celebrated': 256,\n",
       " 'december': 257,\n",
       " '25': 258,\n",
       " 'christmas': 259,\n",
       " 'yen': 260,\n",
       " 'legs': 261,\n",
       " 'spider': 262,\n",
       " 'sport': 263,\n",
       " 'uses': 264,\n",
       " 'net,': 265,\n",
       " 'ball,': 266,\n",
       " 'hoop': 267,\n",
       " 'basketball': 268,\n",
       " 'kangaroos': 269,\n",
       " 'female': 270,\n",
       " 'minister': 271,\n",
       " 'uk': 272,\n",
       " 'margaretthatcher': 273,\n",
       " 'fastest': 274,\n",
       " 'animal': 275,\n",
       " 'cheetah': 276,\n",
       " 'periodic': 277,\n",
       " 'table': 278,\n",
       " 'spain': 279,\n",
       " 'madrid': 280,\n",
       " 'closest': 281,\n",
       " 'sun': 282,\n",
       " 'father': 283,\n",
       " 'computers': 284,\n",
       " 'charlesbabbage': 285,\n",
       " 'mexico': 286,\n",
       " 'mexicocity': 287,\n",
       " 'colors': 288,\n",
       " 'rainbow': 289,\n",
       " 'musical': 290,\n",
       " 'instrument': 291,\n",
       " 'black': 292,\n",
       " 'white': 293,\n",
       " 'keys': 294,\n",
       " 'piano': 295,\n",
       " 'americas': 296,\n",
       " '1492': 297,\n",
       " 'christophercolumbus': 298,\n",
       " 'disney': 299,\n",
       " 'character': 300,\n",
       " 'long': 301,\n",
       " 'nose': 302,\n",
       " 'grows': 303,\n",
       " 'it': 304,\n",
       " 'when': 305,\n",
       " 'lying': 306,\n",
       " 'pinocchio': 307,\n",
       " 'directed': 308,\n",
       " 'movie': 309,\n",
       " 'titanic': 310,\n",
       " 'jamescameron': 311,\n",
       " 'superhero': 312,\n",
       " 'also': 313,\n",
       " 'dark': 314,\n",
       " 'knight': 315,\n",
       " 'batman': 316,\n",
       " 'brasilia': 317,\n",
       " 'fruit': 318,\n",
       " 'king': 319,\n",
       " 'fruits': 320,\n",
       " 'mango': 321,\n",
       " 'eiffel': 322,\n",
       " 'tower': 323}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the vocabulary\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of the vocabulary\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting words to numerical indices\n",
    "def convert_words_to_indices(text, vocab):\n",
    "\n",
    "  indexed_text = []\n",
    "\n",
    "  for word in tokenizer(text):\n",
    "    if word in vocab:\n",
    "      indexed_text.append(vocab[word])\n",
    "    else:\n",
    "      indexed_text.append(vocab['<UNK>'])\n",
    "\n",
    "  return indexed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 0]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the function\n",
    "convert_words_to_indices('What is the capital of Pakistan?', vocab) # Pakistan is not in the vocabulary so it will be replaced with <UNK> = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "  # constructor\n",
    "  def __init__(self, df, vocab):\n",
    "    self.df = df # this will store the dataframe\n",
    "    self.vocab = vocab\n",
    "\n",
    "  # __len__ method\n",
    "  def __len__(self):\n",
    "    return self.df.shape[0] # this will return the number of rows in the dataframe\n",
    "  \n",
    "  # __getitem__ method\n",
    "  def __getitem__(self, index):\n",
    "    num_question = convert_words_to_indices(self.df.iloc[index]['question'], self.vocab) # this will return a list of indices\n",
    "    num_answer = convert_words_to_indices(self.df.iloc[index]['answer'], self.vocab)\n",
    "\n",
    "    return torch.tensor(num_question), torch.tensor(num_answer) # this will return a tuple of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a custom dataset object\n",
    "dataset = CustomDataset(df, vocab) # passing the dataframe and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True) # passing the dataset and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10,  75, 208]]) tensor([[209]])\n",
      "tensor([[10,  2,  3, 66,  5, 67]]) tensor([[68]])\n",
      "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([[244]])\n",
      "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([[154]])\n",
      "tensor([[10, 29,  3, 30, 31]]) tensor([[32]])\n",
      "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([[128]])\n",
      "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([[321]])\n",
      "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([[28]])\n",
      "tensor([[ 10,  11, 189, 158, 190]]) tensor([[191]])\n",
      "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([[106]])\n",
      "tensor([[ 10,  29, 130, 131]]) tensor([[132]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([[99]])\n",
      "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([[156]])\n",
      "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([[36]])\n",
      "tensor([[ 10,  11, 157, 158, 159]]) tensor([[160]])\n",
      "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([[205]])\n",
      "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([[114]])\n",
      "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([[188]])\n",
      "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([[254]])\n",
      "tensor([[ 42, 101,   2,   3,  17]]) tensor([[102]])\n",
      "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([[238]])\n",
      "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([[246]])\n",
      "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([[41]])\n",
      "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([[194]])\n",
      "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([[215]])\n",
      "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([[233]])\n",
      "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([[6]])\n",
      "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([[205]])\n",
      "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([[36]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([[100]])\n",
      "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([[162]])\n",
      "tensor([[ 10, 308,   3, 309, 310]]) tensor([[311]])\n",
      "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([[23]])\n",
      "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([[173]])\n",
      "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([[95]])\n",
      "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([[295]])\n",
      "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([[52]])\n",
      "tensor([[10, 11, 12, 13, 14, 15]]) tensor([[16]])\n",
      "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([[121]])\n",
      "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([[298]])\n",
      "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([[49]])\n",
      "tensor([[1, 2, 3, 4, 5, 8]]) tensor([[9]])\n",
      "tensor([[ 10,  96,   3, 104, 239]]) tensor([[240]])\n",
      "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([[259]])\n",
      "tensor([[ 10,  75, 111]]) tensor([[112]])\n",
      "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([[36]])\n",
      "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([[170]])\n",
      "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([[61]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([[53]])\n",
      "tensor([[10, 55,  3, 56,  5, 57]]) tensor([[58]])\n",
      "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([[225]])\n",
      "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([[72]])\n",
      "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([[185]])\n",
      "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([[199]])\n",
      "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([[91]])\n",
      "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([[113]])\n",
      "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([[316]])\n",
      "tensor([[10, 75, 76]]) tensor([[77]])\n",
      "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([[124]])\n",
      "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([[136]])\n",
      "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([[179]])\n",
      "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([[307]])\n",
      "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([[131]])\n",
      "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([[287]])\n",
      "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([[166]])\n",
      "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([[134]])\n",
      "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([[249]])\n",
      "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([[184]])\n",
      "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([[260]])\n",
      "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([[207]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([[74]])\n",
      "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([[285]])\n",
      "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([[155]])\n",
      "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([[85]])\n",
      "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([[121]])\n",
      "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([[276]])\n",
      "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([[116]])\n",
      "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([[317]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) tensor([[7]])\n",
      "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([[85]])\n",
      "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([[268]])\n",
      "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([[273]])\n",
      "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([[145]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([[54]])\n",
      "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([[280]])\n",
      "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([[220]])\n",
      "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([[65]])\n",
      "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([[149]])\n",
      "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([[110]])\n",
      "tensor([[10, 96,  3, 97]]) tensor([[98]])\n"
     ]
    }
   ],
   "source": [
    "# testing the dataloader\n",
    "for question, answer in dataloader:\n",
    "  print(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a RNN model class\n",
    "class RNN(nn.Module):\n",
    "\n",
    "  # constructor\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__() # calling the constructor of the parent class\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim=50) # creating an embedding layer\n",
    "    self.rnn = nn.RNN(50, 64, batch_first=True)  # creating a recurrent neural network, adding batch_first=True\n",
    "    self.linear = nn.Linear(64, vocab_size) # creating a linear layer\n",
    "\n",
    "  # forward pass\n",
    "  def forward(self, question):\n",
    "    embedded_question = self.embedding(question) # passing the question to the embedding layer\n",
    "    hidden, final = self.rnn(embedded_question) # passing the embedded question to the recurrent neural network\n",
    "    output = self.linear(final.squeeze(0)) # passing the final output to the linear layer and squeezing the batch dimension\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model object\n",
    "model = RNN(len(vocab)) # passing the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # this will be used for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # this will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of question: torch.Size([1, 6])\n",
      "shape of embedded question: torch.Size([1, 6, 50])\n",
      "shape of hidden: torch.Size([1, 6, 64])\n",
      "shape of final: torch.Size([1, 1, 64])\n",
      "shape of output: torch.Size([1, 324])\n"
     ]
    }
   ],
   "source": [
    "# for debugging the training loop\n",
    "x = nn.Embedding(324, embedding_dim=50)\n",
    "y = nn.RNN(50, 64, batch_first=True) # batch_first=True means that the batch dimension is the first dimension.\n",
    "z = nn.Linear(64, 324)\n",
    "\n",
    "a = dataset[0][0].reshape(1, 6) # reshaping the question to 1x6\n",
    "print(\"shape of question:\", a.shape)\n",
    "\n",
    "b = x(a) # passing the question to the embedding layer\n",
    "print(\"shape of embedded question:\", b.shape)\n",
    "\n",
    "c, d = y(b) # passing the embedded question to the recurrent neural network\n",
    "print(\"shape of hidden:\", c.shape)\n",
    "print(\"shape of final:\", d.shape)\n",
    "\n",
    "e = z(d.squeeze(0)) # passing the final output to the linear layer, squeeze(0) means that the batch dimension is removed.\n",
    "print(\"shape of output:\", e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 5.800900014241536\n",
      "Epoch: 2, Loss: 5.006183179219564\n",
      "Epoch: 3, Loss: 4.110608008172777\n",
      "Epoch: 4, Loss: 3.448542594909668\n",
      "Epoch: 5, Loss: 2.891661432054308\n",
      "Epoch: 6, Loss: 2.3753606107499863\n",
      "Epoch: 7, Loss: 1.905985372596317\n",
      "Epoch: 8, Loss: 1.4882905814382765\n",
      "Epoch: 9, Loss: 1.1534574574894376\n",
      "Epoch: 10, Loss: 0.8875230507718193\n",
      "Epoch: 11, Loss: 0.6879603597852919\n",
      "Epoch: 12, Loss: 0.5393921674953567\n",
      "Epoch: 13, Loss: 0.4292439606454637\n",
      "Epoch: 14, Loss: 0.34695204099019367\n",
      "Epoch: 15, Loss: 0.28532734190424286\n",
      "Epoch: 16, Loss: 0.2372521021299892\n",
      "Epoch: 17, Loss: 0.19985258844163684\n",
      "Epoch: 18, Loss: 0.16833765490187538\n",
      "Epoch: 19, Loss: 0.1458765564693345\n",
      "Epoch: 20, Loss: 0.12476225102113353\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  total_loss = 0 # variable to store the total loss of the epoch\n",
    "\n",
    "  for question, answer in dataloader: # iterating over the dataloader\n",
    "\n",
    "    optimizer.zero_grad() # zero the gradients\n",
    "\n",
    "    # forward pass\n",
    "    output = model(question) # passing the question to the model\n",
    "\n",
    "    # loss\n",
    "    loss = criterion(output, answer[0]) # passing the output and the answer [0]\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward() # this will calculate the gradients\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step() # this will update the weights\n",
    "\n",
    "    total_loss += loss.item() # this will store the total loss of the epoch\n",
    "\n",
    "  print(f'Epoch: {epoch + 1}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a predict function\n",
    "def predict(model, question, threshold=0.5): # threshold is the probability threshold\n",
    "\n",
    "  # converting the question to numerical indices\n",
    "  num_question = convert_words_to_indices(question, vocab) # this will return a list of indices\n",
    "\n",
    "  # converting the list of indices to a tensor\n",
    "  num_question = torch.tensor(num_question).unsqueeze(0) # this will return a tensor of shape (6,)\n",
    "\n",
    "  # sending the question to the model\n",
    "  output = model(num_question)\n",
    "\n",
    "  # converting logits to probabilities\n",
    "  probs = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "  # find index of max probability\n",
    "  value, index = torch.max(probs, dim=1)\n",
    "\n",
    "  # check if the probability is greater than the threshold\n",
    "  if value < threshold:\n",
    "    return \"I don't know that one\"\n",
    "\n",
    "  print(list(vocab.keys())[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know that one\""
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the predict function\n",
    "predict(model, 'What is the capital of Pakistan?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# testing another question\n",
    "predict(model, 'What is the boiling point of water in Celsius?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexander-graham-bell\n"
     ]
    }
   ],
   "source": [
    "# testing another question\n",
    "predict(model, 'Who invented the telephone?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
